{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #Detección de caras con webcam usando deepface\n",
    "\n",
    "Deepface crea directorios para descargar los modelos, cuidado si tienes el disco bastante lleno. Es posible configurar la ruta, en mi caso uso  E:\\RUNNERS_code\\code\\DeepFace, tras definir la ruta a través de la variable de entorno DEEPFACE_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "# documentation https://github.com/serengil/deepface/blob/master/deepface/modules/detection.py\n",
    "# detector_backends deepface options: 'opencv', 'retinaface', 'mtcnn', 'ssd', 'dlib', 'mediapipe', 'yolov8', 'centerface' or 'skip'\n",
    "detectors = ['opencv', 'mtcnn', 'retinaface', 'ssd']\n",
    "detector_idx = 0\n",
    "\n",
    "# Webcam connection\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Read frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Face detection\n",
    "        faces = DeepFace.extract_faces(frame, detector_backend=detectors[detector_idx])\n",
    "        #print(faces)\n",
    "\n",
    "        # Draw face bounding box and eyes locations\n",
    "        for face in faces:\n",
    "            x, y, w, h = face['facial_area']['x'], face['facial_area']['y'], face['facial_area']['w'], face['facial_area']['h']\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (200, 255, 200), 2)  # Dibuja el rectángulo\n",
    "            if face['facial_area']['left_eye'] is not None:\n",
    "                cv2.circle(frame, (face['facial_area']['left_eye'][0],face['facial_area']['left_eye'][1]), 3, (0, 255, 0), 2)\n",
    "                cv2.circle(frame, (face['facial_area']['right_eye'][0],face['facial_area']['right_eye'][1]), 3, (0, 0, 255), 2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    # Mostrar el frame\n",
    "    cv2.putText(frame, f\"Detector: {detectors[detector_idx]}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "\n",
    "    # Salir si se presiona Esc o cambia el detectr\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:  # Salir si se presiona 'q'\n",
    "        break\n",
    "    elif key == ord('d'):  # Cambiar detector si se presiona 'c'\n",
    "        detector_idx = (detector_idx + 1) % len(detectors)\n",
    "        print(f\"Cambiado a detector: {detectors[detector_idx]}\")\n",
    "\n",
    "# Liberar la captura y cerrar ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deepface y Retinaface únicamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from retinaface import RetinaFace\n",
    "\n",
    "# Load the mask image (ensure it has an alpha channel)\n",
    "mask_img = cv2.imread('mask.png', -1)\n",
    "# Remove the flip unless necessary\n",
    "mask_img = cv2.flip(mask_img, 0)\n",
    "\n",
    "# Connect to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    if not cap.isOpened():\n",
    "        print('Error de cámara')\n",
    "        exit(0)\n",
    "    else:\n",
    "        print('Cámara 1')\n",
    "else:\n",
    "    print('Cámara 0')\n",
    "\n",
    "# Set camera resolution\n",
    "cap.set(3, 640)\n",
    "cap.set(4, 480)\n",
    "\n",
    "while True:\n",
    "    # Get the frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Detect faces\n",
    "    faces = RetinaFace.detect_faces(frame)\n",
    "    if len(faces) > 0:\n",
    "        for key in faces.keys():\n",
    "            face = faces[key]\n",
    "            landmarks = face['landmarks']\n",
    "\n",
    "            # Get positions of eyes, nose, and mouth\n",
    "            left_eye = landmarks[\"left_eye\"]\n",
    "            right_eye = landmarks[\"right_eye\"]\n",
    "            nose = landmarks[\"nose\"]\n",
    "            mouth_left = landmarks[\"mouth_left\"]\n",
    "            mouth_right = landmarks[\"mouth_right\"]\n",
    "            facial_area = face['facial_area']\n",
    "\n",
    "            # Draw the reference points\n",
    "            cv2.circle(frame, (int(left_eye[0]), int(left_eye[1])), 3, (0, 255, 0), -1)\n",
    "            cv2.circle(frame, (int(right_eye[0]), int(right_eye[1])), 3, (0, 255, 0), -1)\n",
    "            cv2.circle(frame, (int(nose[0]), int(nose[1])), 3, (0, 255, 0), -1)\n",
    "            cv2.circle(frame, (int(mouth_left[0]), int(mouth_left[1])), 3, (0, 255, 0), -1)\n",
    "            cv2.circle(frame, (int(mouth_right[0]), int(mouth_right[1])), 3, (0, 255, 0), -1)\n",
    "            cv2.circle(frame, (int(facial_area[0]), int(facial_area[1])), 3, (0, 255, 0), -1)\n",
    "\n",
    "            # Corrected rectangle drawing\n",
    "            cv2.rectangle(frame, (int(facial_area[0]), int(facial_area[1])),\n",
    "                                 (int(facial_area[2]), int(facial_area[3])), (0, 255, 0), 2)\n",
    "\n",
    "            # Calculate the angle between the eyes (in degrees)\n",
    "            delta_x = right_eye[0] - left_eye[0]\n",
    "            delta_y = right_eye[1] - left_eye[1]\n",
    "            angle = np.degrees(np.arctan2(delta_y, delta_x))\n",
    "\n",
    "            # Calculate the center between the eyes\n",
    "            eyes_center = ((left_eye[0] + right_eye[0]) / 2,\n",
    "                           (left_eye[1] + right_eye[1]) / 2)\n",
    "\n",
    "            # Calculate face width based on the distance between the mouth corners\n",
    "            face_width = np.linalg.norm(np.array(mouth_right) - np.array(mouth_left))\n",
    "\n",
    "            # Adjust mask size to match the face width\n",
    "            mask_width = int(face_width * 1.2)  # Adjust scaling factor as needed\n",
    "            mask_height = int(mask_width * mask_img.shape[0] / mask_img.shape[1])  # Maintain aspect ratio\n",
    "\n",
    "            # Ensure mask dimensions are valid\n",
    "            if mask_width > 0 and mask_height > 0:\n",
    "                # Resize the mask image\n",
    "                resized_mask = cv2.resize(mask_img, (mask_width, mask_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                # Rotate the mask image around its center\n",
    "                M = cv2.getRotationMatrix2D((mask_width / 2, mask_height / 2), angle, 1)\n",
    "                rotated_mask = cv2.warpAffine(resized_mask, M, (mask_width, mask_height), flags=cv2.INTER_AREA, borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0,0))\n",
    "\n",
    "                # Calculate the position to overlay the mask\n",
    "                x1 = int(eyes_center[0] - mask_width / 2)\n",
    "                y1 = int(eyes_center[1] - mask_height / 3)  # Adjust the vertical position as needed\n",
    "                x2 = x1 + mask_width\n",
    "                y2 = y1 + mask_height\n",
    "\n",
    "                # Adjust coordinates to fit within the frame\n",
    "                x1_mask = 0\n",
    "                y1_mask = 0\n",
    "                x2_mask = mask_width\n",
    "                y2_mask = mask_height\n",
    "\n",
    "                if x1 < 0:\n",
    "                    x1_mask = -x1\n",
    "                    x1 = 0\n",
    "                if y1 < 0:\n",
    "                    y1_mask = -y1\n",
    "                    y1 = 0\n",
    "                if x2 > frame.shape[1]:\n",
    "                    x2_mask = mask_width - (x2 - frame.shape[1])\n",
    "                    x2 = frame.shape[1]\n",
    "                if y2 > frame.shape[0]:\n",
    "                    y2_mask = mask_height - (y2 - frame.shape[0])\n",
    "                    y2 = frame.shape[0]\n",
    "\n",
    "                # Ensure integer values\n",
    "                x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "                x1_mask, y1_mask, x2_mask, y2_mask = map(int, [x1_mask, y1_mask, x2_mask, y2_mask])\n",
    "\n",
    "                # Extract the mask region\n",
    "                mask_to_overlay = rotated_mask[y1_mask:y2_mask, x1_mask:x2_mask]\n",
    "\n",
    "                # Check if the mask has an alpha channel\n",
    "                if mask_to_overlay.shape[2] == 4:\n",
    "                    alpha_mask = mask_to_overlay[:, :, 3] / 255.0\n",
    "                    alpha_inv = 1.0 - alpha_mask\n",
    "                    mask_rgb = mask_to_overlay[:, :, :3]\n",
    "\n",
    "                    # Get the region of interest from the frame\n",
    "                    roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "                    # Combine the mask with the ROI\n",
    "                    for c in range(0, 3):\n",
    "                        roi[:, :, c] = (alpha_mask * mask_rgb[:, :, c] +\n",
    "                                        alpha_inv * roi[:, :, c])\n",
    "\n",
    "                    # Place the combined ROI back into the frame\n",
    "                    frame[y1:y2, x1:x2] = roi\n",
    "                else:\n",
    "                    print(\"La imagen de la máscara no tiene un canal alfa.\")\n",
    "            else:\n",
    "                print(\"Dimensiones de la máscara no válidas.\")\n",
    "\n",
    "    # Show the resulting frame\n",
    "    cv2.imshow('Cam', frame)\n",
    "\n",
    "    # Exit loop when 'Esc' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from retinaface import RetinaFace\n",
    "\n",
    "# Load the mask image\n",
    "mask_img = cv2.imread('mask.png', -1)  # Ensure this path is correct\n",
    "\n",
    "# Webcam connection\n",
    "cap = cv2.VideoCapture(0)\n",
    "# ... [Your existing camera setup code] ...\n",
    "\n",
    "while True:\n",
    "    # Get frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Detect faces\n",
    "    faces = RetinaFace.detect_faces(frame)\n",
    "    if len(faces) > 0:\n",
    "        for idx in range(1, len(faces)+1):\n",
    "            id = 'face_' + str(idx)\n",
    "            landmarks = faces[id]['landmarks']\n",
    "\n",
    "            # Get positions of the eyes\n",
    "            left_eye = landmarks[\"left_eye\"]\n",
    "            right_eye = landmarks[\"right_eye\"]\n",
    "\n",
    "            # Calculate the center between eyes\n",
    "            eye_center = (\n",
    "                int((left_eye[0] + right_eye[0]) / 2),\n",
    "                int((left_eye[1] + right_eye[1]) / 2)\n",
    "            )\n",
    "\n",
    "            # Calculate angle between the eyes\n",
    "            delta_x = right_eye[0] - left_eye[0]\n",
    "            delta_y = right_eye[1] - left_eye[1]\n",
    "            angle = np.degrees(np.arctan2(delta_y, delta_x))\n",
    "\n",
    "            # Calculate the distance between the eyes\n",
    "            eye_distance = np.hypot(delta_x, delta_y)\n",
    "\n",
    "            # Determine the size of the mask\n",
    "            mask_width = int(eye_distance * 2)  # Adjust scaling factor as needed\n",
    "            mask_height = int(mask_width * mask_img.shape[0] / mask_img.shape[1])\n",
    "\n",
    "            # Resize the mask image\n",
    "            resized_mask = cv2.resize(mask_img, (mask_width, mask_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Rotate the mask image\n",
    "            M = cv2.getRotationMatrix2D((mask_width / 2, mask_height / 2), angle, 1)\n",
    "            rotated_mask = cv2.warpAffine(resized_mask, M, (mask_width, mask_height), flags=cv2.INTER_AREA)\n",
    "\n",
    "            # Calculate position to overlay the mask\n",
    "            x1 = int(eye_center[0] - mask_width / 2)\n",
    "            y1 = int(eye_center[1] - mask_height / 2)\n",
    "\n",
    "            # Ensure the coordinates are within the frame\n",
    "            x1 = max(0, x1)\n",
    "            y1 = max(0, y1)\n",
    "            x2 = min(frame.shape[1], x1 + mask_width)\n",
    "            y2 = min(frame.shape[0], y1 + mask_height)\n",
    "\n",
    "            # Adjust the mask size if it goes beyond the frame\n",
    "            rotated_mask = rotated_mask[0:(y2 - y1), 0:(x2 - x1)]\n",
    "\n",
    "            # Extract the alpha mask and BGR channels from the mask image\n",
    "            if rotated_mask.shape[2] == 4:\n",
    "                alpha_mask = rotated_mask[:, :, 3] / 255.0\n",
    "                alpha_inv = 1.0 - alpha_mask\n",
    "                mask_bgr = rotated_mask[:, :, :3]\n",
    "\n",
    "                # Get the region of interest from the frame\n",
    "                roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "                # Blend the mask with the ROI\n",
    "                for c in range(0, 3):\n",
    "                    roi[:, :, c] = (alpha_mask * mask_bgr[:, :, c] + alpha_inv * roi[:, :, c])\n",
    "\n",
    "                # Place the blended ROI back into the frame\n",
    "                frame[y1:y2, x1:x2] = roi\n",
    "            else:\n",
    "                print(\"Mask image does not have an alpha channel.\")\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Cam', frame)\n",
    "\n",
    "    # Break the loop on 'Esc' key press\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from retinaface import RetinaFace\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Load visual effects images\n",
    "effects = {\n",
    "    'happy': cv2.imread('sunglasses.png', -1),\n",
    "    'sad': cv2.imread('rain.png', -1),\n",
    "    'surprise': cv2.imread('wow.png', -1),\n",
    "    'angry': cv2.imread('flames.png', -1),\n",
    "    # Add more emotions and corresponding images as needed\n",
    "}\n",
    "\n",
    "# Webcam connection\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Get frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect faces\n",
    "    faces = RetinaFace.detect_faces(frame)\n",
    "    if faces:\n",
    "        for idx, (key, face_data) in enumerate(faces.items()):\n",
    "            # Extract facial region\n",
    "            facial_area = face_data['facial_area']\n",
    "            x, y, w, h = facial_area[0], facial_area[1], facial_area[2]-facial_area[0], facial_area[3]-facial_area[1]\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "\n",
    "            # Emotion recognition\n",
    "            try:\n",
    "                analysis = DeepFace.analyze(face_img, actions=['emotion'], enforce_detection=False)\n",
    "                emotion = analysis['dominant_emotion']\n",
    "            except:\n",
    "                emotion = 'neutral'\n",
    "\n",
    "            # Select effect based on emotion\n",
    "            effect_img = effects.get(emotion)\n",
    "            if effect_img is not None:\n",
    "                # Resize and position the effect\n",
    "                effect_resized = cv2.resize(effect_img, (w, h))\n",
    "                alpha_s = effect_resized[:, :, 3] / 255.0\n",
    "                alpha_l = 1.0 - alpha_s\n",
    "\n",
    "                for c in range(0, 3):\n",
    "                    frame[y:y+h, x:x+w, c] = (alpha_s * effect_resized[:, :, c] +\n",
    "                                              alpha_l * frame[y:y+h, x:x+w, c])\n",
    "            else:\n",
    "                # Optional: handle neutral or unrecognized emotions\n",
    "                pass\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Emotion-Responsive Filters', frame)\n",
    "\n",
    "    # Break the loop on 'Esc' key press\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from retinaface import RetinaFace\n",
    "\n",
    "# Webcam connection\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def add_sparkles(frame, center, num_sparkles=10, color=(0, 255, 255)):\n",
    "    \"\"\"Add sparkles around a given center point on the frame.\"\"\"\n",
    "    for _ in range(num_sparkles):\n",
    "        offset_x = np.random.randint(-30, 30)\n",
    "        offset_y = np.random.randint(-30, 30)\n",
    "        cv2.circle(frame, (center[0] + offset_x, center[1] + offset_y), 2, color, -1)\n",
    "\n",
    "def enlarge_eyes(frame, eye_center, size=1.5):\n",
    "    \"\"\"Enlarge eye regions to create a surprised look.\"\"\"\n",
    "    x, y = eye_center\n",
    "    w, h = 30, 20  # Width and height around the eye center\n",
    "    roi = frame[y-h:y+h, x-w:x+w]\n",
    "    if roi.shape[0] > 0 and roi.shape[1] > 0:  # Check if ROI is within bounds\n",
    "        enlarged_eye = cv2.resize(roi, None, fx=size, fy=size, interpolation=cv2.INTER_LINEAR)\n",
    "        frame[y-h:y+h, x-w:x+w] = enlarged_eye[:h*2, :w*2]  # Place resized eye back in frame\n",
    "\n",
    "def apply_red_tint(frame):\n",
    "    \"\"\"Apply a red tint to the frame.\"\"\"\n",
    "    frame[:, :, 2] = cv2.add(frame[:, :, 2], 50)\n",
    "\n",
    "def add_tears(frame, eye_center):\n",
    "    \"\"\"Draw tears below the eye to simulate a sad expression.\"\"\"\n",
    "    x, y = eye_center\n",
    "    for i in range(5):\n",
    "        cv2.circle(frame, (x, y + 15 + i*10), 5, (255, 0, 0), -1)  # Draw blue tear circles\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect faces and get landmarks\n",
    "    faces = RetinaFace.detect_faces(frame)\n",
    "    if faces:\n",
    "        for face in faces.values():\n",
    "            landmarks = face['landmarks']\n",
    "            print(landmarks)\n",
    "            left_eye = np.array(landmarks['left_eye'], np.int32)\n",
    "            right_eye = np.array(landmarks['right_eye'], np.int32)\n",
    "            mouth_left = np.array(landmarks['mouth_left'], np.int32)\n",
    "            mouth_right = np.array(landmarks['mouth_right'], np.int32)\n",
    "            # mouth_top = np.array(landmarks['mouth_up'], np.int32)\n",
    "            # mouth_bottom = np.array(landmarks['mouth_down'], np.int32)\n",
    "            \n",
    "            # Calculate mouth aspect ratio (MAR) to detect smile\n",
    "            mar = np.linalg.norm(mouth_left - mouth_right) / np.linalg.norm(left_eye - right_eye)\n",
    "\n",
    "            # Calculate eye aspect ratio (EAR) to detect surprise\n",
    "            eye_aspect_ratio = np.linalg.norm(left_eye - right_eye)\n",
    "\n",
    "            # Detect expressions and apply effects\n",
    "            if True:  # Happy expression detected\n",
    "                center_mouth = tuple((mouth_left + mouth_right) // 2)\n",
    "                add_sparkles(frame, center_mouth)\n",
    "                cv2.putText(frame, 'Happy', (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            elif True:  # Surprised expression detected\n",
    "                enlarge_eyes(frame, left_eye)\n",
    "                enlarge_eyes(frame, right_eye)\n",
    "                cv2.putText(frame, 'Surprised', (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            elif True:  # Angry expression detected (small MAR indicates tight lips)\n",
    "                apply_red_tint(frame)\n",
    "                cv2.putText(frame, 'Angry', (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            elif True:  # Sad expression detected\n",
    "                add_tears(frame, left_eye)\n",
    "                add_tears(frame, right_eye)\n",
    "                cv2.putText(frame, 'Sad', (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            else:  # Neutral expression, apply grayscale filter\n",
    "                gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                frame = cv2.merge([gray_frame, gray_frame, gray_frame])\n",
    "                cv2.putText(frame, 'Neutral', (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Real-Time Expression Manipulation', frame)\n",
    "\n",
    "    # Break the loop on 'Esc' key press\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Detect faces every 'detection_interval' frames\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_count \u001b[38;5;241m%\u001b[39m detection_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 89\u001b[0m     faces \u001b[38;5;241m=\u001b[39m \u001b[43mRetinaFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Analyze emotions every 'emotion_interval' frames\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_count \u001b[38;5;241m%\u001b[39m emotion_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/VC_P5/lib/python3.11/site-packages/retinaface/RetinaFace.py:123\u001b[0m, in \u001b[0;36mdetect_faces\u001b[0;34m(img_path, threshold, model, allow_upscaling)\u001b[0m\n\u001b[1;32m    121\u001b[0m landmarks_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    122\u001b[0m im_tensor, im_info, im_scale \u001b[38;5;241m=\u001b[39m preprocess\u001b[38;5;241m.\u001b[39mpreprocess_image(img, allow_upscaling)\n\u001b[0;32m--> 123\u001b[0m net_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m net_out \u001b[38;5;241m=\u001b[39m [elt\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m elt \u001b[38;5;129;01min\u001b[39;00m net_out]\n\u001b[1;32m    125\u001b[0m sym_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/VC_P5/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/VC_P5/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/VC_P5/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/VC_P5/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/VC_P5/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/VC_P5/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VC_P5/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/VC_P5/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/VC_P5/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from retinaface import RetinaFace\n",
    "from deepface import DeepFace  # Added DeepFace import\n",
    "\n",
    "# Debug mode\n",
    "debug = True  # Set to False to disable debug mode\n",
    "\n",
    "# Load filter images with alpha channel\n",
    "glasses = cv2.imread('gafas.png', cv2.IMREAD_UNCHANGED)\n",
    "mustache = cv2.imread('bigote.png', cv2.IMREAD_UNCHANGED)\n",
    "hat = cv2.imread('sombrero.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Verify if filter images are loaded correctly\n",
    "if glasses is None or mustache is None or hat is None:\n",
    "    print(\"Error loading filter images.\")\n",
    "    exit()\n",
    "\n",
    "# Connect to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize variables for FPS calculation\n",
    "fps = 0\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize variables for face detection frequency\n",
    "detection_interval = 3  # Detect faces every 3 frames\n",
    "emotion_interval = 5  # Analyze emotions every 5 frames\n",
    "faces = None\n",
    "emotion_results = {}\n",
    "\n",
    "# Flags to indicate which filters are active\n",
    "use_glasses = True\n",
    "use_mustache = True\n",
    "use_hat = True\n",
    "\n",
    "def overlay_image(frame, overlay, position):\n",
    "    \"\"\"Overlay an image with transparency over the frame.\"\"\"\n",
    "    x, y = position\n",
    "    h, w = overlay.shape[:2]\n",
    "\n",
    "    # Ensure the coordinates are within the frame\n",
    "    if x < 0 or y < 0 or x + w > frame.shape[1] or y + h > frame.shape[0]:\n",
    "        return\n",
    "\n",
    "    # Get region of interest on the frame\n",
    "    roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "    # Separate the color and alpha channels\n",
    "    overlay_img = overlay[:, :, :3]\n",
    "    overlay_mask = overlay[:, :, 3]\n",
    "\n",
    "    # Convert mask to 3 channels and normalize\n",
    "    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR) / 255.0\n",
    "    background_mask = 1.0 - overlay_mask\n",
    "\n",
    "    # Blend the overlay with the ROI\n",
    "    roi = (overlay_img * overlay_mask + roi * background_mask).astype(np.uint8)\n",
    "\n",
    "    # Put the blended image back into the frame\n",
    "    frame[y:y+h, x:x+w] = roi\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Increment frame count\n",
    "    frame_count += 1\n",
    "\n",
    "    # Calculate FPS every 10 frames\n",
    "    if frame_count % 10 == 0:\n",
    "        end_time = time.time()\n",
    "        fps = 10 / (end_time - start_time)\n",
    "        start_time = end_time\n",
    "\n",
    "    if debug:\n",
    "        # Display FPS on the frame\n",
    "        cv2.putText(frame, f'FPS: {fps:.2f}', (10, 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "    # Detect faces every 'detection_interval' frames\n",
    "    if frame_count % detection_interval == 0:\n",
    "        faces = RetinaFace.detect_faces(frame)\n",
    "\n",
    "    # Analyze emotions every 'emotion_interval' frames\n",
    "    if frame_count % emotion_interval == 0:\n",
    "        analyze_emotions = True\n",
    "    else:\n",
    "        analyze_emotions = False\n",
    "\n",
    "    face_count = 0\n",
    "    if isinstance(faces, dict):\n",
    "        face_count = len(faces)\n",
    "        for face_id, face in faces.items():\n",
    "            facial_area = face['facial_area']\n",
    "            landmarks = face['landmarks']\n",
    "            x1, y1, x2, y2 = facial_area\n",
    "            face_roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "            # Perform emotion analysis\n",
    "            if analyze_emotions:\n",
    "                try:\n",
    "                    analysis = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "                    \n",
    "                    if isinstance(analysis, list):\n",
    "                        analysis = analysis[0]\n",
    "                        \n",
    "                    emotion = analysis['dominant_emotion']\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Emotion analysis failed: {e}\")\n",
    "                    emotion_results[face_id] = 'Unknown'\n",
    "            else:\n",
    "                emotion = emotion_results.get(face_id, 'Analyzing...')\n",
    "\n",
    "            if debug:\n",
    "                # Display the emotion label on the frame\n",
    "                cv2.putText(frame, f'Emotion: {emotion}', (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "                # Draw bounding rectangle\n",
    "                face_width = x2 - x1\n",
    "                face_height = y2 - y1\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2),\n",
    "                              (255, 0, 0), 2)\n",
    "                cv2.putText(frame, f'Face: ({x1},{y1}) ({x2},{y2}) W:{face_width} H:{face_height}',\n",
    "                            (x1, y2 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0), 1)\n",
    "\n",
    "                # Display landmarks with names and coordinates\n",
    "                for point_name, point in landmarks.items():\n",
    "                    x, y = int(point[0]), int(point[1])\n",
    "                    cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "                    cv2.putText(frame, f'{point_name} ({x},{y})', (x + 5, y - 5),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)\n",
    "\n",
    "            # Calculate positions and sizes for filters\n",
    "            left_eye = landmarks['left_eye']\n",
    "            right_eye = landmarks['right_eye']\n",
    "            nose = landmarks['nose']\n",
    "            mouth_left = landmarks['mouth_left']\n",
    "            mouth_right = landmarks['mouth_right']\n",
    "\n",
    "            eye_center = ((left_eye[0] + right_eye[0]) / 2,\n",
    "                          (left_eye[1] + right_eye[1]) / 2)\n",
    "            eye_width = int(abs(right_eye[0] - left_eye[0]) * 2)\n",
    "            eye_height = int(eye_width * glasses.shape[0] / glasses.shape[1])\n",
    "            eye_x = int(eye_center[0] - eye_width / 2)\n",
    "            eye_y = int(eye_center[1] - eye_height / 2)\n",
    "\n",
    "            # Adjust position and size of mustache\n",
    "            mouth_center = ((mouth_left[0] + mouth_right[0]) / 2,\n",
    "                            (mouth_left[1] + mouth_right[1]) / 2)\n",
    "            mustache_width = int(abs(mouth_right[0] - mouth_left[0]) * 1.5)\n",
    "            mustache_height = int(\n",
    "                mustache_width * mustache.shape[0] / mustache.shape[1])\n",
    "            mustache_x = int(mouth_center[0] - mustache_width / 2)\n",
    "            mustache_y = int(mouth_center[1] - mustache_height / 1.4)\n",
    "\n",
    "            # Calculate position and size of hat\n",
    "            head_width = int(eye_width * 1.5)\n",
    "            head_height = int(\n",
    "                head_width * hat.shape[0] / hat.shape[1])\n",
    "            head_x = int(eye_center[0] - head_width / 2)\n",
    "            head_y = int(eye_y - head_height + eye_height // 2)\n",
    "\n",
    "            # Resize and overlay filters based on user selection\n",
    "            if use_glasses:\n",
    "                glasses_resized = cv2.resize(\n",
    "                    glasses, (eye_width, eye_height), interpolation=cv2.INTER_AREA)\n",
    "                overlay_image(frame, glasses_resized, (eye_x, eye_y))\n",
    "\n",
    "            if use_mustache:\n",
    "                mustache_resized = cv2.resize(\n",
    "                    mustache, (mustache_width, mustache_height), interpolation=cv2.INTER_AREA)\n",
    "                overlay_image(frame, mustache_resized, (mustache_x, mustache_y))\n",
    "\n",
    "            if use_hat:\n",
    "                hat_resized = cv2.resize(\n",
    "                    hat, (head_width, head_height), interpolation=cv2.INTER_AREA)\n",
    "                overlay_image(frame, hat_resized, (head_x, head_y))\n",
    "\n",
    "    if debug:\n",
    "        # Display number of faces detected\n",
    "        cv2.putText(frame, f'Faces detected: {face_count}', (10, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "    # Show the result\n",
    "    cv2.imshow('Fun Filter', frame)\n",
    "\n",
    "    # Handle key events\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:\n",
    "        # Exit with 'Esc' key\n",
    "        break\n",
    "    elif key == ord('d'):\n",
    "        # Toggle debug mode with 'd' key\n",
    "        debug = not debug\n",
    "    elif key == ord('g'):\n",
    "        # Toggle glasses filter with 'g' key\n",
    "        use_glasses = not use_glasses\n",
    "    elif key == ord('m'):\n",
    "        # Toggle mustache filter with 'm' key\n",
    "        use_mustache = not use_mustache\n",
    "    elif key == ord('h'):\n",
    "        # Toggle hat filter with 'h' key\n",
    "        use_hat = not use_hat\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from deepface import DeepFace\n",
    "from retinaface import RetinaFace\n",
    "import random\n",
    "\n",
    "# Debug mode\n",
    "debug = True  # Set to False to disable debug mode\n",
    "\n",
    "# Load funny images with alpha channel\n",
    "funny_images = [\n",
    "    cv2.imread('balloon.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('confetti.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('laugh_emoji.png', cv2.IMREAD_UNCHANGED)\n",
    "]\n",
    "\n",
    "# Verify if funny images are loaded correctly\n",
    "for idx, img in enumerate(funny_images):\n",
    "    if img is None:\n",
    "        print(f\"Error loading image {idx}\")\n",
    "        exit()\n",
    "\n",
    "# Connect to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize variables for FPS calculation\n",
    "fps = 0\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize variables for animated images\n",
    "animated_images = []\n",
    "\n",
    "def overlay_image(background, overlay, position):\n",
    "    \"\"\"Overlay an image with transparency over the background.\"\"\"\n",
    "    x, y = position\n",
    "    h, w = overlay.shape[:2]\n",
    "\n",
    "    # Ensure the coordinates are within the frame\n",
    "    if x < 0 or y < 0 or x + w > background.shape[1] or y + h > background.shape[0]:\n",
    "        return\n",
    "\n",
    "    # Get region of interest\n",
    "    roi = background[y:y+h, x:x+w]\n",
    "\n",
    "    # Separate channels\n",
    "    overlay_img = overlay[:, :, :3]\n",
    "    overlay_mask = overlay[:, :, 3:] / 255.0\n",
    "\n",
    "    background_mask = 1.0 - overlay_mask\n",
    "\n",
    "    # Blend the images\n",
    "    roi = (overlay_img * overlay_mask + roi * background_mask).astype(np.uint8)\n",
    "\n",
    "    background[y:y+h, x:x+w] = roi\n",
    "\n",
    "# Initialize background subtractor for person contour detection\n",
    "backSub = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Increment frame count\n",
    "    frame_count += 1\n",
    "\n",
    "    # Calculate FPS every 10 frames\n",
    "    if frame_count % 10 == 0:\n",
    "        end_time = time.time()\n",
    "        fps = 10 / (end_time - start_time)\n",
    "        start_time = end_time\n",
    "\n",
    "    if debug:\n",
    "        # Display FPS on the frame\n",
    "        cv2.putText(frame, f'FPS: {fps:.2f}', (10, 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = RetinaFace.detect_faces(frame)\n",
    "    face_count = 0\n",
    "    emotion = None\n",
    "\n",
    "    if isinstance(faces, dict):\n",
    "        face_count = len(faces)\n",
    "        for key in faces:\n",
    "            face = faces[key]\n",
    "            facial_area = face['facial_area']\n",
    "            x1, y1, x2, y2 = facial_area\n",
    "\n",
    "            # Extract face ROI\n",
    "            face_roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "            # Emotion detection using DeepFace\n",
    "            try:\n",
    "                analysis = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "                emotion = analysis['dominant_emotion']\n",
    "                if debug:\n",
    "                    cv2.putText(frame, f'Emotion: {emotion}', (x1, y1 - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"Emotion detection error: {e}\")\n",
    "\n",
    "            # Draw bounding box around the face in debug mode\n",
    "            if debug:\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2),\n",
    "                              (255, 0, 0), 2)\n",
    "\n",
    "            # Process only the first detected face\n",
    "            break\n",
    "\n",
    "    # Person contour detection using background subtraction\n",
    "    fg_mask = backSub.apply(frame)\n",
    "    _, fg_mask = cv2.threshold(fg_mask, 244, 255, cv2.THRESH_BINARY)\n",
    "    fg_mask = cv2.erode(fg_mask, None, iterations=2)\n",
    "    fg_mask = cv2.dilate(fg_mask, None, iterations=2)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    person_mask = np.zeros_like(frame[:, :, 0])\n",
    "\n",
    "    if contours:\n",
    "        # Assume the largest contour is the person\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        cv2.drawContours(person_mask, [largest_contour], -1, 255, cv2.FILLED)\n",
    "\n",
    "    # Invert person mask to get background mask\n",
    "    background_mask = cv2.bitwise_not(person_mask)\n",
    "\n",
    "    # Create background where we'll place the animated images\n",
    "    background = cv2.bitwise_and(frame, frame, mask=background_mask)\n",
    "\n",
    "    # Update animated images positions\n",
    "    new_animated_images = []\n",
    "    for img_info in animated_images:\n",
    "        img = img_info['image']\n",
    "        x = img_info['x']\n",
    "        y = img_info['y'] - img_info['speed']  # Move up by 'speed' pixels\n",
    "\n",
    "        # If the image is still within the frame, draw it\n",
    "        if y + img.shape[0] > 0:\n",
    "            overlay_image(background, img, (x, y))\n",
    "            img_info['y'] = y\n",
    "            new_animated_images.append(img_info)\n",
    "    animated_images = new_animated_images\n",
    "\n",
    "    # Add new images based on emotion\n",
    "    if emotion in ['happy', 'surprise', 'neutral'] and frame_count % 30 == 0:\n",
    "        # Randomly select a funny image\n",
    "        img = random.choice(funny_images)\n",
    "        img_h, img_w = img.shape[:2]\n",
    "\n",
    "        # Random x position within the frame width\n",
    "        x = random.randint(0, frame.shape[1] - img_w)\n",
    "        y = frame.shape[0]  # Start from the bottom\n",
    "        speed = random.randint(2, 5)  # Random speed for variation\n",
    "\n",
    "        animated_images.append({'image': img, 'x': x, 'y': y, 'speed': speed})\n",
    "\n",
    "    # Combine background with person\n",
    "    person = cv2.bitwise_and(frame, frame, mask=person_mask)\n",
    "    frame_result = cv2.add(background, person)\n",
    "\n",
    "    if debug:\n",
    "        # Display number of faces detected\n",
    "        cv2.putText(frame_result, f'Faces detected: {face_count}', (10, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "    # Show the result\n",
    "    cv2.imshow('Emotion Filter', frame_result)\n",
    "\n",
    "    # Handle key events\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:\n",
    "        # Exit with 'Esc' key\n",
    "        break\n",
    "    elif key == ord('d'):\n",
    "        # Toggle debug mode with 'd' key\n",
    "        debug = not debug\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
